\documentclass[12pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage[table]{xcolor}
\usepackage[margin=2cm]{geometry}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{mathtools}
\usepackage{palatino}
\usepackage[brazil]{babel}
\newlength{\tabcont}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

\begin{document}
	
	\textbf{Nome}: Luís Felipe de Melo Costa Silva \\
	\textbf{Número USP}: 9297961 
	
	\begin{center}
		\LARGE \bf
		Lista de Exercícios 4 - MAE0228
	\end{center}
	
	\section*{Exercício 1}
	
	\textbf{a)} Vamos definir $Y_i$ como o número de insetos gerados pelo $i$-ésimo ovo. Logo, 
	
	 \begin{equation*}
		 Y_i=
			 \begin{cases}
			 1, & \text{com probabilidade } p  \\
			 0, & \text{c. c.}
			 \end{cases}
	 \end{equation*}
	 
	 Pode-se observar que $Y_i \sim$ Bernoulli$(p)$. Sendo N a variável aleatória de distribuição Poisson$(\lambda)$ que determina o número de ovos colocados pelo inseto-mãe, podemos definir $X$ como uma partição da seguinte maneira:
	 
	 \begin{center}
	 	$X =\sum_{i=1}^{N} Y_i$
	 \end{center}
	 
	 \textbf{b)} Usando a definição, a função geradora de momentos de $X$ é:
	 
	 \begin{equation*}
		 \begin{split}
			 M_X(t) & = G_N(M_{Y_i}(t)) = G_N(E(e^{tY_i})) = G_N(1 - p + pe^t) = \\
			 & = E((1 - p + pe^t)^N) = \sum_{k=0}^{\infty} (1 - p + pe^t)^i \cdot \frac{e^{-\lambda} \lambda^i}{i!} = \\
			 & = e^{-\lambda} \sum_{k=0}^{\infty} \frac{[(1 - p + pe^t)\cdot\lambda]^i}{i!} = e^{-\lambda} e^{(1 - p + pe^t)\cdot\lambda} = e^{\lambda p (e^t -1)}
		 \end{split}
	 \end{equation*}
	 
	 Logo, $X \sim$ Poisson$(\lambda p)$. Com isso, sabemos que a função geradora de probabilidade de $X$ é $G_x(t) = e^{\lambda p (t -1)}$.
	 
	 \textbf{c)} Para calcular $ E(X) $ e $ Var(X) $, vamos usar os momentos. Sabemos que $ E(X^i) $, o $ i $-ésimo momento da variável $ X $, é a $ i $-ésima derivada de $ M_X(t) $ com relação a $ t $. Com isso:
	 
	 \begin{equation*}
	 	E(X) = M_x'(t) \rvert_{t=0} = (e^{\lambda p (e^t -1)})'\rvert_{t=0} = (e^{\lambda p (e^t -1)} \cdot \lambda pe^t) \rvert_{t=0} = \lambda p
	 \end{equation*}
	 
	 Agora, para a variância, precisamos de $ E(X^2) $.
	 
	 \begin{equation*}
		 \begin{split}
			 E(X^2) & =  M_x''(t) \rvert_{t=0} = (e^{\lambda p (e^t -1)})''\rvert_{t=0} = (e^{\lambda p (e^t -1)} \cdot \lambda pe^t)'\rvert_{t=0} = \\
			 & = (e^{\lambda p (e^t -1)} \cdot \lambda pe^t + e^{\lambda p (e^t -1)} \cdot \lambda pe^t \cdot \lambda pe^t)\rvert_{t=0} = \lambda p + (\lambda p)^2
		 \end{split}
	 \end{equation*}
	 
	 Portanto, $ Var(X) = \lambda p + (\lambda p)^2 - (\lambda p)^2 = \lambda p $

	\section*{Exercício 3}
	
	\textbf{b)} Sendo $X_1, X_2, ..., X_k$ variáveis aleatórias independentes, com $X_i \sim \text{Binomial}(n_i, p)$, teremos:
	
	\begin{equation*}
		\begin{split}
			M_{X_1 + ... + X_k}(t) & = \prod_{i=1}^{k} M_{X_i}(t) = \prod_{i=1}^{k} (1-p+p\cdot e^t)^{n_i} = (1-p+p\cdot e^t)^{\sum_{i=1}^{k} n_i} = (1-p+p\cdot e^t)^{n_1+n_2+...+n_k} 
		\end{split}
	\end{equation*}
	
	Logo, $X_1 + X_2 + ... + X_k \sim \text{Binomial}(n_1+n_2+...+n_k,p)$ \\ 
	
	\textbf{d)} Sendo $X_1, X_2, ..., X_r$ variáveis aleatórias independentes, com $X_i \sim \text{Exponencial}(\lambda)$, teremos:
	
	\begin{equation*}
		\begin{split}
			M_{X_1 + ... + X_r}(t) & = \prod_{i=1}^{r} M_{X_i}(t) = \prod_{i=1}^{r} \frac{\lambda}{\lambda - t} = \left(\frac{\lambda}{\lambda - t}\right)^r
		\end{split}
	\end{equation*}
	
	Logo, $X_1 + X_2 + ... + X_r \sim \text{Gama}(r, \lambda)$ 
	
	\section*{Exercício 5}
	
	Sendo $Y$ uma variável aleatória discreta:
	
	\begin{equation*}
		\begin{split}
			M_Y(t) & = E(e^{tY}) = \int_{-\infty}^{+\infty} e^{ty} \cdot f_Y(y)\text{d}y = \int_{-\infty}^{u} e^{ty} \cdot f_Y(y)\text{d}y + \int_{u}^{+\infty} e^{ty} \cdot f_Y(y)\text{d}y \\
			& \geq \int_{u}^{+\infty} e^{ty} \cdot f_Y(y)\text{d}y \geq \int_{u}^{+\infty} e^{tu} \cdot f_Y(y)\text{d}y \\
			& \geq e^{tu} \cdot \int_{u}^{+\infty} f_Y(y)\text{d}y = e^{ut} \cdot P(Y > u)\\
		\end{split}
	\end{equation*}
	
	Portanto,
	
	\begin{center}
		$e^{ut} \cdot P(Y > u) \leq M_Y(t)$ \\
		$P(Y > u) \leq e^{-ut} \cdot M_Y(t)$ \\
	\end{center}
	 
	\qed 
	 
\end{document}