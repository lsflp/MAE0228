\documentclass[12pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage[table]{xcolor}
\usepackage[margin=2cm]{geometry}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{mathtools}
\usepackage{palatino}
\usepackage[brazil]{babel}
\newlength{\tabcont}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}

\begin{document}
	
	\textbf{Nome}: Luís Felipe de Melo Costa Silva \\
	\textbf{Número USP}: 9297961 
	
	\begin{center}
		\LARGE \bf
		Lista de Exercícios 4 - MAE0228
	\end{center}
	
	\section*{Exercício 1}
	
	\textbf{a)} Vamos definir $Y_i$ como o número de insetos gerados pelo $i$-ésimo ovo. Logo, 
	
	 \begin{equation*}
		 Y_i=
			 \begin{cases}
			 1, & \text{com probabilidade } p  \\
			 0, & \text{c. c.}
			 \end{cases}
	 \end{equation*}
	 
	 Pode-se observar que $Y_i \sim$ Bernoulli$(p)$. Sendo N a variável aleatória de distribuição Poisson$(\lambda)$ que determina o número de ovos colocados pelo inseto-mãe, podemos definir $X$ como uma partição da seguinte maneira:
	 
	 \begin{center}
	 	$X =\sum_{i=1}^{N} Y_i$
	 \end{center}
	 
	 \textbf{b)} Usando a definição, a função geradora de momentos de $X$ é:
	 
	 \begin{equation*}
		 \begin{split}
			 M_X(t) & = G_N(M_{Y_i}(t)) = G_N(E(e^{tY_i})) = G_N(1 - p + pe^t) = \\
			 & = E((1 - p + pe^t)^N) = \sum_{k=0}^{\infty} (1 - p + pe^t)^i \cdot \frac{e^{-\lambda} \lambda^i}{i!} = \\
			 & = e^{-\lambda} \sum_{k=0}^{\infty} \frac{[(1 - p + pe^t)\cdot\lambda]^i}{i!} = e^{-\lambda} e^{(1 - p + pe^t)\cdot\lambda} = e^{\lambda p (e^t -1)}
		 \end{split}
	 \end{equation*}
	 
	 Logo, $X \sim$ Poisson$(\lambda p)$. Com isso, sabemos que a função geradora de probabilidade de $X$ é $G_x(t) = e^{\lambda p (t -1)}$.
	 
	 \textbf{c)} Para calcular $ E(X) $ e $ Var(X) $, vamos usar os momentos. Sabemos que $ E(X^i) $, o $ i $-ésimo momento da variável $ X $, é a $ i $-ésima derivada de $ M_X(t) $ com relação a $ t $. Com isso:
	 
	 \begin{equation*}
	 	E(X) = M_x'(t) \rvert_{t=0} = (e^{\lambda p (e^t -1)})'\rvert_{t=0} = (e^{\lambda p (e^t -1)} \cdot \lambda pe^t) \rvert_{t=0} = \lambda p
	 \end{equation*}
	 
	 Agora, para a variância, precisamos de $ E(X^2) $.
	 
	 \begin{equation*}
		 \begin{split}
			 E(X^2) & =  M_x''(t) \rvert_{t=0} = (e^{\lambda p (e^t -1)})''\rvert_{t=0} = (e^{\lambda p (e^t -1)} \cdot \lambda pe^t)'\rvert_{t=0} = \\
			 & = (e^{\lambda p (e^t -1)} \cdot \lambda pe^t + e^{\lambda p (e^t -1)} \cdot \lambda pe^t \cdot \lambda pe^t)\rvert_{t=0} = \lambda p + (\lambda p)^2
		 \end{split}
	 \end{equation*}
	 
	 Portanto, $ Var(X) = \lambda p + (\lambda p)^2 - (\lambda p)^2 = \lambda p $

	\section*{Exercício 3}
	
	\textbf{b)} Sendo $X_1, X_2, ..., X_k$ variáveis aleatórias independentes, com $X_i \sim \text{Binomial}(n_i, p)$, teremos:
	
	\begin{equation*}
		\begin{split}
			M_{X_1 + ... + X_k}(t) & = \prod_{i=1}^{k} M_{X_i}(t) = \prod_{i=1}^{k} (1-p+p\cdot e^t)^{n_i} = (1-p+p\cdot e^t)^{\sum_{i=1}^{k} n_i} = (1-p+p\cdot e^t)^{n_1+n_2+...+n_k} 
		\end{split}
	\end{equation*}
	
	Logo, $X_1 + X_2 + ... + X_k \sim \text{Binomial}(n_1+n_2+...+n_k,p)$ \\ 
	
	\textbf{d)} Sendo $X_1, X_2, ..., X_r$ variáveis aleatórias independentes, com $X_i \sim \text{Exponencial}(\lambda)$, teremos:
	
	\begin{equation*}
		\begin{split}
			M_{X_1 + ... + X_r}(t) & = \prod_{i=1}^{r} M_{X_i}(t) = \prod_{i=1}^{r} \frac{\lambda}{\lambda - t} = \left(\frac{\lambda}{\lambda - t}\right)^r
		\end{split}
	\end{equation*}
	
	Logo, $X_1 + X_2 + ... + X_r \sim \text{Gama}(r, \lambda)$ 
	
	\section*{Exercício 4}
	
	\textbf{a)} Queremos $ Z_n = \text{min}\{X_1, X_2, ..., X_n\} $. Vamos chamar de $ U_1, U_2, ..., U_n $ as mesmas variáveis, só que ordenadas do menor para o maior. Logo, o que estamos procurando é $ P(U_1 < z) $, que é $ 1-P(U_1 \geq z) $. Então:
	
	\begin{equation*}
		\begin{split}
		P(U_1 < z) & = 1 - P(X_1 \geq z, X_2 \geq z, ..., X_n \geq z) = 1 - \left[ (1 - F_{X_1}(z)) \cdot (1 - F_{X_2}(z)) \cdot ... \cdot (1 - F_{X_n}(z)) \right] \\
		& = 1 - \left[ (1 - (1 - e^{-\lambda_1 z})) \cdot (1 - (1 - e^{-\lambda_2 z})) \cdot ... \cdot (1 - (1 - e^{-\lambda_n z})) \right] \\
		& = 1 - \left[ (e^{-\lambda_1 z}) \cdot (e^{-\lambda_2 z}) \cdot ... \cdot (e^{-\lambda_n z}) \right] = 1 - e^{-(\lambda_1+\lambda_2+...+\lambda_n)z}
		\end{split}
	\end{equation*} 
	
	$ Z_n \sim \text{Exponencial}(\lambda_1+\lambda_2+...+\lambda_n) $ \\
	
	\textbf{b)} Para um $ X_k $ qualquer ser um mínimo, fazemos:
	
	\begin{equation*}
		\begin{split}
			P(Z_n=X_k) & = \int_{0}^{\infty} P(Z_n = x_k | X_k = x_k) \cdot P(X_k = x_k) \text{d}x_k = \\
			& = \int_{0}^{\infty} \frac{P(\bigcap_{i=0}^{n} Xi \geq x_k \bigcap X_k = x_k)}{P(X_k = x_k)} \cdot P(X_k = x_k) \text{d}x_k  = \int_{0}^{\infty} P\left(\bigcap_{i=0}^{n} Xi \geq x_k \bigcap X_k = x_k\right) \text{d}x_k  \\
			& \stackrel{ind.}{=} \int_{0}^{\infty} \prod_{i \neq k} P(Xi \geq x_k) \cdot P(X_k=x_k) \text{d}x_k = \int_{0}^{\infty} \prod_{i \neq k} \left[ \int_{x_k}^{\infty} \lambda_i e^{-\lambda_ix_i}\text{d}x_i \right] \cdot (\lambda_k e^{-\lambda_k x_k}) \text{d}x_k \\
			& = \int_{0}^{\infty} \prod_{i \neq k} [1 - e^{-\lambda_i x_i} \rvert_{x_k}^{\infty} ] \cdot (\lambda_k e^{-\lambda_k x_k}) \text{d}x_k = \int_{0}^{\infty} \prod_{i \neq k} (e^{-\lambda_i x_k}) \cdot (\lambda_k e^{-\lambda_k x_k}) \text{d}x_k \\
			& = \lambda_k \int_{0}^{\infty} e^{-\sum_{i=0}^{n} \lambda_i x_k}\text{d}x_k = -\lambda_k \frac{e^{\sum_{i=0}^{n} \lambda_i x_k} |_0^\infty}{\sum_{i=0}^{n} \lambda_i} = \frac{\lambda_k}{\lambda_1 + \lambda_2 + ... +\lambda_n} 
		\end{split}
	\end{equation*}
	
	\section*{Exercício 5}
	
	Sendo $Y$ uma variável aleatória contínua:
	
	\begin{equation*}
		\begin{split}
			M_Y(t) & = E(e^{tY}) = \int_{-\infty}^{+\infty} e^{ty} \cdot f_Y(y)\text{d}y = \int_{-\infty}^{u} e^{ty} \cdot f_Y(y)\text{d}y + \int_{u}^{+\infty} e^{ty} \cdot f_Y(y)\text{d}y \\
			& \geq \int_{u}^{+\infty} e^{ty} \cdot f_Y(y)\text{d}y \geq \int_{u}^{+\infty} e^{tu} \cdot f_Y(y)\text{d}y \\
			& \geq e^{tu} \cdot \int_{u}^{+\infty} f_Y(y)\text{d}y = e^{ut} \cdot P(Y > u)\\
		\end{split}
	\end{equation*}
	
	Portanto,
	
	\begin{center}
		$e^{ut} \cdot P(Y > u) \leq M_Y(t)$ \\
		$P(Y > u) \leq e^{-ut} \cdot M_Y(t)$ \\
	\end{center}
	 
	\qed 
	 
\end{document}